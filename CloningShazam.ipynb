{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XebjioDCCRBJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb sentence-transformers openai-whisper faiss-cpu\n"
      ],
      "metadata": {
        "id": "BHdF5GjRdhhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import zipfile\n",
        "import io\n",
        "import pandas as pd\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import chromadb\n",
        "import whisper\n",
        "import faiss\n",
        "import sqlite3\n"
      ],
      "metadata": {
        "id": "W5RlQ_HIKkKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# File Paths\n",
        "DB_PATH = \"/content/drive/MyDrive/Colab Notebooks/Major/eng_subtitles_database.db\"#database path\n",
        "AUDIO_PATH = \"/content/drive/MyDrive/Colab Notebooks/Major/query_audio.mp3\"#input audio file path"
      ],
      "metadata": {
        "id": "5n_cy6Mm_JCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_subtitles(db_path, sample_fraction=0.3):\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(\"SELECT num, name, content FROM zipfiles\")\n",
        "    rows = cursor.fetchall()\n",
        "    conn.close()\n",
        "\n",
        "    subtitles = []\n",
        "    for num, name, content in rows:\n",
        "        try:\n",
        "            # Decompress ZIP content\n",
        "            with zipfile.ZipFile(io.BytesIO(content), 'r') as z:\n",
        "                for filename in z.namelist():\n",
        "                    with z.open(filename) as f:\n",
        "                        text = f.read().decode('latin-1')  # Decode properly\n",
        "                        subtitles.append({'id': num, 'name': name, 'content': text})\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting {name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    df = pd.DataFrame(subtitles)\n",
        "    return df.sample(frac=sample_fraction, random_state=42)  # Reduce dataset to 30%\n"
      ],
      "metadata": {
        "id": "ghnfFg-x0ZUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing subtitles\n",
        "def clean_subtitles(text):\n",
        "    text = re.sub(r'\\d{2}:\\d{2}:\\d{2},\\d{3} --> \\d{2}:\\d{2}:\\d{2},\\d{3}', '', text)  # Remove timestamps\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove special characters\n",
        "     # Remove subtitle line numbers\n",
        "    text = re.sub(r'^\\d+\\s*$', '', text, flags=re.MULTILINE)\n",
        "    # Remove extra spaces and newlines\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n"
      ],
      "metadata": {
        "id": "DVvMlL003CXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Document Chunking\n",
        "def chunk_text(text, chunk_size=500, overlap=50):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), chunk_size - overlap):\n",
        "        chunk = ' '.join(words[i:i+chunk_size])\n",
        "        chunks.append(chunk)\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "OpBd81KK3pug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to TF-IDF\n",
        "def tfidf_vectorization(docs):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    vectors = vectorizer.fit_transform(docs)\n",
        "    return vectorizer, vectors"
      ],
      "metadata": {
        "id": "SSCQHx1B0_HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to SentenceTransformer Embeddings\n",
        "def embed_sentences(sentences, model_name='all-MiniLM-L6-v2'):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    embeddings = model.encode(sentences, convert_to_tensor=True)\n",
        "    return model, embeddings"
      ],
      "metadata": {
        "id": "SyEgh1wq1Byy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store embeddings in FAISS\n",
        "def store_embeddings_faiss(embeddings):\n",
        "    d = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(d)\n",
        "    faiss.normalize_L2(embeddings)\n",
        "    index.add(embeddings)\n",
        "    return index\n"
      ],
      "metadata": {
        "id": "abHj0a6x1ilF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process Audio Query\n",
        "def transcribe_audio(audio_path):\n",
        "    model = whisper.load_model(\"base\")\n",
        "    result = model.transcribe(audio_path)\n",
        "    return result['text']"
      ],
      "metadata": {
        "id": "sPmysSdi_tLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform FAISS Search\n",
        "def search_query_faiss(query, model, index, embeddings, docs):\n",
        "    query_embedding = model.encode([query])\n",
        "    faiss.normalize_L2(query_embedding)\n",
        "    _, indices = index.search(query_embedding, 5)  # Top 5 results\n",
        "    return [docs[i] for i in indices[0]]"
      ],
      "metadata": {
        "id": "LiJ-tmDb_wtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Printing Output Headers\n",
        "    def print_section(title, icon):\n",
        "        print(\"\\n==============================\")\n",
        "        print(f\"{icon} \\033[1m{title}\\033[0m\")\n",
        "        print(\"==============================\\n\")\n",
        "\n",
        "    print_section(\"Loading Subtitles...\", \"üìå\")\n",
        "    df = load_subtitles(DB_PATH)\n",
        "    df['clean_content'] = df['content'].apply(clean_subtitles)\n",
        "\n",
        "    print_section(\"Applying Text Chunking...\", \"‚úÇÔ∏è\")\n",
        "    df['chunks'] = df['clean_content'].apply(lambda x: chunk_text(x))\n",
        "    df = df.explode('chunks').reset_index(drop=True)\n",
        "\n",
        "    print_section(\"Generating BERT Embeddings...\", \"üß†\")\n",
        "    model, embeddings = embed_sentences(df['chunks'].tolist())\n",
        "\n",
        "    print_section(\"Storing Embeddings in FAISS...\", \"üíæ\")\n",
        "    faiss_index = store_embeddings_faiss(embeddings.cpu().numpy())\n",
        "\n",
        "    print_section(\"Processing Audio Query...\", \"üéôÔ∏è\")\n",
        "    query_text = transcribe_audio(AUDIO_PATH)\n"
      ],
      "metadata": {
        "id": "5K6h9RSE_zPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Step 6: Perform Search\n",
        "print_section(\"Performing Search...\", \"üîç\")\n",
        "results = search_query_faiss(query_text, model, faiss_index, embeddings.cpu().numpy(), df['chunks'].tolist())\n",
        "print(\"üîé Search Completed! Top Results:\\n\")\n",
        "for i, res in enumerate(results[:5]):\n",
        "    print(f\"{i+1}. {res}\\n\")"
      ],
      "metadata": {
        "id": "ca7beHr1Ww72"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}